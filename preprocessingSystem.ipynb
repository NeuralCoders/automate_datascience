{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessingSystem.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<p><img alt=\"Yahoo logo\" height=\"45px\" src=\"data:image/png;base64,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\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1>We make data science easy</h1>"
      ],
      "metadata": {
        "id": "3slg3GK9L82S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Código protegido por las licencias de Neural Coders S.A. "
      ],
      "metadata": {
        "id": "WxQ39JluLwvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uso UNICAMENTE INTERNO."
      ],
      "metadata": {
        "id": "EJE4inm6Z8sh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<h1>Data Science Applications for Blockper Software</h1>\n",
        "<p>Blockper es un software dedicado a simplificar las aplicaciones de la Ciencia de Datos de una forma sencilla, simple y fácil de aprender a usar. \n",
        "\n",
        "<p>Este notebook recopila algoritmos de suma importancia para data cleaning, data sanitization y Machine Learning.</p>"
      ],
      "metadata": {
        "id": "0qVaAtmt_1HZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <hr width=1000>"
      ],
      "metadata": {
        "id": "FR-_aN_oM6Ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Data Cleaning<h1> \n",
        "\n",
        "El data cleaning es un conjunto de metodologías y prácticas utilizadas para convertir datos pocor servibles y dificiles de trabajar, en datos que nos proporcienen información sobre el negocios, asi como también, permitan su facil manejo y pruebas de modelos. "
      ],
      "metadata": {
        "id": "OP2OE8TRQr1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1 Importación de librerías"
      ],
      "metadata": {
        "id": "rEjMAvwPvUiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip"
      ],
      "metadata": {
        "id": "hht3LxgKytON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFrdIOrb11ta"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas_profiling import ProfileReport\n",
        "from sklearn import preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 Lectura de Dataset\n",
        "En esta sección se desarrollan dos formas de lectura de dataset. En primer lugar la lectura mediante Open, función propia de Python. En segundo lugar, la lectura convencional con la librería Pandas."
      ],
      "metadata": {
        "id": "whbQZnNUp6zs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1 Reading Dataset (Open)"
      ],
      "metadata": {
        "id": "I1aH9kW6sGWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_df_open(path):\n",
        "  df = open(path,\"r\")\n",
        "  cols = df.readline().strip().split(\",\")\n",
        "  n_cols = len(cols)\n",
        "  count = 0\n",
        "  main_dict = {}\n",
        "\n",
        "  for col in cols:\n",
        "    main_dict[col] = []\n",
        "  \n",
        "  for line in df:\n",
        "    values = line.strip().split(\",\")\n",
        "    for i in range(len(cols)):\n",
        "      main_dict[cols[i]].append(values[i])\n",
        "  count += 1\n",
        "  df = pd.DataFrame(main_dict)\n",
        "  return df"
      ],
      "metadata": {
        "id": "-R-B844D2HbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 Reading Dataset (Pandas)"
      ],
      "metadata": {
        "id": "ViMS7gl0vpIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_df_pandas(path):\n",
        "  '''Esta funcion recibe como parametro la direccion en donde se encuentra el archivo a leer. Retorna el dataframe.'''\n",
        "\n",
        "  option = int(input(\"Wich type dataset? \\n 1) CSV \\n 2) SXLS \\n 3) SQL \\n\"))\n",
        "\n",
        "  if(option==1):\n",
        "    df = pd.read_csv(path)\n",
        "  elif(option==2):\n",
        "    df = pd.read_excel(path)\n",
        "  else:\n",
        "    df = pd.read_sql_table(path)\n",
        "\n",
        "  return df\n",
        "\n",
        "#----------------------------------#\n",
        "\n",
        "df = read_df_pandas(\"iris.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muGgN17iux7Y",
        "outputId": "eb50cdf4-6e2f-4657-e8e0-1dcacd8c1a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wich type dataset? \n",
            " 1) CSV \n",
            " 2) SXLS \n",
            " 3) SQL \n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3 Funciones para documentación y profiling de dataset. \n",
        "Acá se encuentran múltiples funciones de importancias para mantener documentados y con información el Dataset cargado. "
      ],
      "metadata": {
        "id": "eLpbkKbM0I9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3.1 Funcion para documentar Dataset."
      ],
      "metadata": {
        "id": "ifbXZB8L3vxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_info_df():\n",
        "  '''Esta funcion permite añadir mucha información a los dataset que guardamos en la plataforma. Retorna un diccionario con toda la info'''\n",
        "\n",
        "  info={\"Nombre\":\"\",\"Descripcion\":\"\",\"Categoria\":\"\"}\n",
        "  cat_option=10\n",
        "  categoria=\"\"\n",
        "\n",
        "  Nombre = input(\"Define un nombre para tu Dataset: \\n\")\n",
        "  info[\"Nombre\"]=Nombre\n",
        "\n",
        "  descripcion = input(\"Agrega una descripción a tu dataset: \\n\")\n",
        "  info[\"Descripcion\"]=descripcion\n",
        "\n",
        "  while(cat_option!=5):\n",
        "    cat_option = int(input(\"¿En cuál categoria clasificas tu dataset? \\n 1) Financiero. \\n 2) Ventas. \\n 3) Clientes. \\n 4) Demográfico \\n 5) No tiene.\"))\n",
        "    if (cat_option==1):\n",
        "      categoria=\"Financiero\"\n",
        "    elif(cat_option==2):\n",
        "      categoria=\"Ventas\"\n",
        "    elif(cat_option==3):\n",
        "      categoria=\"Clientes\"\n",
        "    elif(cat_option==4):\n",
        "      categoria=\"Demografico\"\n",
        "    info[\"Categoria\"]=categoria\n",
        "  return info\n",
        "  \n",
        "    "
      ],
      "metadata": {
        "id": "LzBHcifP4tjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3.2 Función para mostrar información sobre el dataset"
      ],
      "metadata": {
        "id": "CtPN0bcUPIV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3.2.1 Funciones generales para mostrar información"
      ],
      "metadata": {
        "id": "kAtgRlxMoyDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_types_colums(df):\n",
        "  '''Funcion que permite conocer los tipos de datos de cada columna'''\n",
        "  return df.dtypes\n",
        "\n",
        "def null_values(df):\n",
        "  '''Funcion que permite conocer el conteo de nulos dentro del dataset'''  \n",
        "  return df.isnull().sum()\n",
        "\n",
        "def duplicate_values(df):\n",
        "  '''Funcion que permite conocer el conteo de duplicados dentro del dataset''' \n",
        "  return df.duplicated().sum()\n",
        "\n",
        "def data_summary(df):\n",
        "  '''Retorna el sumario del dataset''' \n",
        "  summary = df.describe()\n",
        "  summary = summary.transpose()\n",
        "  return  summary"
      ],
      "metadata": {
        "id": "q_ZzrLMsiG-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3.2.2 Funcion principal para mostrar información."
      ],
      "metadata": {
        "id": "eFW6UlwCo4rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_info(df):\n",
        "  '''Esta funcion permite conocer informacion general sobre los datos del dataset'''\n",
        "  #La idea es generar un reporte rapido en pdf. Descargable.\n",
        "  msg = \"1) Mostrar los tipos de datos del dataset \\n 2) Mostrar suma de valores nulos \\n 3) Mostrar suma de valores nulos. \\n 4) Sumario de datos. \\n 5) Salir.\"\n",
        "  option = 0\n",
        "\n",
        "  while(option!=5):\n",
        "    print(msg)\n",
        "    option = int(input(\"Ingresa una opción: \\n\"))\n",
        "    if (option==1):\n",
        "      data_types_colums(df)\n",
        "    elif(option==2):\n",
        "      null_values(df)\n",
        "    elif(option==3):\n",
        "      duplicate_values(df)\n",
        "    elif(option==4):\n",
        "      data_summary(df)\n"
      ],
      "metadata": {
        "id": "9VeD-TAZpvWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3.3 Generar reporte de perfil de los datos (Pandas Profiling)."
      ],
      "metadata": {
        "id": "5igQgRLCwmQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3.3.1 Funciones para generar reportes."
      ],
      "metadata": {
        "id": "iE_1Wwmxh0Ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#profile report\n",
        "def profile_report_normal(df):\n",
        "  '''Funcion que permite generar un reporte con pandas profiling. Este reporte será normal y habrá que usar scroll para verlo todo'''\n",
        "  profile = ProfileReport(df, title=\"Pandas Profiling Report\")\n",
        "  profile.to_file(\"your_report.html\") #esto guarda el report en html\n",
        "  return profile\n",
        "\n",
        "def profile_report_widgets(df):\n",
        "  '''Funcion que permite generar un reporte con pandas profiling. Este reporte se puede navegar mediante pestañas (widgets).'''\n",
        "  profile = ProfileReport(df, title=\"Pandas Profiling Report\")\n",
        "  profile = profile.to_widgets() #esto genera los widgets\n",
        "  profile.to_file(\"your_report.html\")#esto guarda el report en html\n",
        "  return profile"
      ],
      "metadata": {
        "id": "xtSkdyVMwSvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####3.3.2 Función generar para generar reportes."
      ],
      "metadata": {
        "id": "coNL9EdPh48K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_profiling(df):\n",
        "  '''La libreria de data profiling tiene Licencia MIT, por lo que veo una oportunidad de simplificar el uso de esta.'''\n",
        "  msg = \"1) Descargar reporte. \\n 2) Descargar reporte con widgets. 3) Salir.\"\n",
        "  option = 0\n",
        "\n",
        "  while(option!=3):\n",
        "    print(msg)\n",
        "    option=int(input(\"Digite una opcion: \"))\n",
        "    if(option==1):\n",
        "      profile_report_normal(df)\n",
        "    elif(option==2):\n",
        "      profile_report_widgets(df)"
      ],
      "metadata": {
        "id": "D_WScLqNhxCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4 Data Cleaning"
      ],
      "metadata": {
        "id": "B0T2RBvb4wUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4.1 Función para eliminar datos nulos, en blanco y duplicados."
      ],
      "metadata": {
        "id": "ciUh83Cl37ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_cleaning(df):\n",
        "  '''Esta funcion permite hacer la respectiva limpieza del dataset. Retorna un df nuevo'''\n",
        "  #Seria ideal normalizar datos y eliminar columnas inservibles.\n",
        "  option = 0\n",
        "  msg = \"1) Eliminar valores nulos del dataset. \\n 2) Eliminar valores duplicados del dataset. \\n 3) Reemplazar valores en blanco por Nulos 4) Reemplazar valores nulos por 0. \\n 5) Salir.\"\n",
        "\n",
        "  while(option!=5):\n",
        "    option=int(input(\"Digite una opción: \"))\n",
        "    if(option==1):\n",
        "      print(\"1) Borrar filas donde exista al menos un valor nulo \\n 2) Borrar filas donde todas las columnas tengan valores nulos\")\n",
        "      option2 = int(input(\"Digite una opcion: \"))\n",
        "      if(option2==1):\n",
        "        new_df = df.dropna(axis=0, how=\"any\")\n",
        "      elif(option==2):\n",
        "        new_df = df.dropna(axis=0, how=\"all\")\n",
        "    elif(option==2):\n",
        "      new_df = df.drop_duplicates()\n",
        "    elif(option==3):\n",
        "      new_df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "    elif(option==4):\n",
        "      new_df = df.fillna(0)\n",
        "    return new_df\n"
      ],
      "metadata": {
        "id": "iLw5Ydj63Gr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4.2 Variables Dummy\n",
        "En esta seccion se presentan una serie de funciones, las cuales permiten encontrar columnas dentro de una dataframe las cuales son objetos, indicio principal para que sea categoricas y por ende, hacer la funcion dummys en ellas."
      ],
      "metadata": {
        "id": "Roz0KSNiToj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4.2.1 Detección de columnas categóricas."
      ],
      "metadata": {
        "id": "bvCu67NufHfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def category_list(df):\n",
        "  '''Esta funcion devuelve una lista de String con las variables que son categoricas o al menos no son int o float.'''\n",
        "  if(len(df.select_dtypes(include=['object']).columns.tolist())>1):\n",
        "    return df.select_dtypes(include=['object']).columns.tolist()\n",
        "  else:\n",
        "    return \"Este dataset no cuenta con variables categoricas\""
      ],
      "metadata": {
        "id": "o-q321Fzdenh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 4.2.2 Deteccion de numero de columnas categoricas"
      ],
      "metadata": {
        "id": "ak4O7kEbEzOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def num_colum_category(df):\n",
        "  '''Esta funcion encuentra las posiciones exactas de las columans que son categoricas'''\n",
        "  num=[]\n",
        "  for i in range(len(df.columns.values)):\n",
        "    if(df[df.columns.values[i]].dtypes == object):\n",
        "      num.append(i)\n",
        "  return num"
      ],
      "metadata": {
        "id": "XzwhbbcIE1JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2.3 Generador de CSV de dummys"
      ],
      "metadata": {
        "id": "CwuLwF9kOCd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dummys_to_csv(df):\n",
        "  '''Esta es la funcion principal que utiliza las dos anterior funciones para generar nuevos CSV.'''\n",
        "  \n",
        "  #los cuales  servirian de dos formas. 1) se analizan los csv por separado como dummys. \n",
        "  #2) se unen esos datos al df original y se eliminan las columnas que no son dommys\n",
        "\n",
        "  for i in range(len(num_colum_category(df))):\n",
        "    dummy = pd.get_dummies(df[df.columns.values[num_colum_category(df)[i]]], prefix=category_list(df)[0])\n",
        "    dummy.to_csv('dummy'+ category_list(df)[i]+\".csv\")"
      ],
      "metadata": {
        "id": "q3UQ8Ex9Nh6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prueba para ver los dummys\n",
        "dummys = pd.read_csv(\"dummySex.csv\", index_col=0) #importante el index\n",
        "dummys"
      ],
      "metadata": {
        "id": "P_SP49suRN2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}